# This file computes the training and testing losses and c-indexes  
# for the LSTM-GLM, the LSTM-NN, and the six dynamic transformed MRL models 
# (baseline, last-value carried forward, average, linear regression, mixed effects, FPCA).
library(dplyr)
library(glmnet)
library(foreach)
library(doParallel)
set.seed(5678)


############################### USER SET-UP ##############################
#Survival model ("aft" or "cox")
surv_model = "aft"

#Prediction time 
t = 5

#Number of iterations
iter = 500

#Number of cores to parallelize across
numCores = 25

#Directory path of folder containing data generated by 01_generate_data.R
##Must contain sub-directories "aft" and "cox"
data_path = "D:/Simulations/data/"

#Directory path of folder containing context vector data sets generated by 02_create_contextVecs.py
##Must contain sub-directories "aft" and "cox"
contextVec_path = "D:/Simulations/context_vectors/"

#Directory path of folder containing linear regression & mixed effects data sets generated by 03_create_linReg_mixedEff.R
##Must contain sub-directories "aft" and "cox"
lm_path = "D:/Simulations/linReg_mixedEff/"

#Directory path of folder containing FPCA data sets generated by 04_create_fpca.R
##Must contain sub-directories "aft" and "cox"
fpca_path = "D:/Simulations/fpca/"

#Directory path of folder containing train/test data sets generated by 05_trainTest_ipcw.R
##Must contain sub-directories "aft" and "cox"
ipcw_path = "D:/Simulations/ipcw/"

#Directory path of folder containing LSTM-NN predictions generated by 06_lstmNN_pred.py
##Must contain sub-directories "aft" and "cox"
lstmNN_path = "D:/Simulations/lstmNN/"

#Directory path of output folder to write results to
##Must contain sub-directories "aft" and "cox"
output_path = "D:/Simulations/results/"


##########################  FUNCTIONS TO COMPUTE C-INDEX ##########################
## Function to compute numerator & denominator summand of c-index for a given pair of patients
#Index of first patient: i
#Index of second patient: j
#Data set of id, true_val, pred_value, delta_star: concDF
update_Concord = function(i, j, concDF) {
  newNum = 0
  newDen = 0
  
  #Calculate summand in numerator & denominator
  if(i!=j) {
    newNum = (concDF$true_value[i] > concDF$true_value[j])*(concDF$pred_value[i] > concDF$pred_value[j])*concDF$delta_star[j]
    newDen = (concDF$true_value[i] > concDF$true_value[j])*concDF$delta_star[j]
  }
  
  return(c(newNum, newDen))
}

## Function to compute c-index
#Data set of id, true_val, pred_value, delta_star: df
compute_Concord = function(df) {
  #Get all pairwise combinations of patient indices (i,j) s.t. delta*_j>0
  combos = expand.grid(1:nrow(df), which(df$delta_star>0))
  
  #Compute numerator & denominator summand of c-index for each i,j
  concordSums = mapply(FUN=update_Concord, combos$Var1, combos$Var2, MoreArgs=list(concDF=df))
  
  #Calculate c-index
  concord = sum(concordSums[1,])/sum(concordSums[2,])
  return(concord)
}


##########################  FUNCTION TO COMPUTE LOSS ########################## 
#Vector of observed MRLs: y_true
#Vector of predicted MRLs: y_pred
#Vector of IPCWs: ipcw
lossFunction = function(y_true, y_pred, ipcw) {
  return( sum(ipcw*(y_true-y_pred)^2)/sum(ipcw) )
}


##########################  FUNCTION TO COMPUTE METRICS FOR LSTM-NN ##########################
#Dataframe containing the training predictions for the LSTM-NN: nnTrain
#Dataframe containing the testing predictions for the LSTM-NN: nnTest
lstmNN_metrics = function(nnTrain, nnTest) {
  #Compute loss
  trainLoss = lossFunction(log(nnTrain$restricted_residual_life), nnTrain$pred, nnTrain$ipcw)
  testLoss = lossFunction(log(nnTest$restricted_residual_life), nnTest$pred, nnTest$ipcw)
  
  #Create training data set for c-index
  error_train = data.frame(id=nnTrain$id, true_value=log(nnTrain$restricted_residual_life),
                           pred_value=nnTrain$pred, delta_star=nnTrain$delta_star)
  
  #Create testing data set for c-index
  error_test = data.frame(id=nnTest$id, true_value=log(nnTest$restricted_residual_life),
                          pred_value=nnTest$pred, delta_star=nnTest$delta_star)
  
  #Compute c-index
  trainConcord = compute_Concord(error_train)
  testConcord = compute_Concord(error_test)
  
  return(list(trainLoss,testLoss,trainConcord,testConcord))
}


##########################  FUNCTION TO COMPUTE METRICS FOR DYNAMIC TRANSFORMED MODEL ##########################
#Dataframe containing the training data for the model: trainReg
#Dataframe containing the testing data for the model: testReg
dynTrans_metrics = function(trainReg, testReg) {
  #Remove response variables & convert to matrix for glmnet
  trainReg_glmnet = data.matrix(subset(trainReg, select=-c(id, ipcw, delta_star, restricted_residual_life)))
  testReg_glmnet = data.matrix(subset(testReg, select=-c(id, ipcw, delta_star, restricted_residual_life)))
  
  #Fit model on training data
  lr = glmnet(x=trainReg_glmnet, y=log(trainReg$restricted_residual_life), family="gaussian",
                 weights=trainReg$ipcw, alpha=1, standardize=F)
  
  #Predict restricted residual life; set lambda=0
  trainPredlr = predict(lr, newx=trainReg_glmnet, s=0)
  testPredlr = predict(lr, newx=testReg_glmnet, s=0)
  
  #Compute loss
  trainLoss = lossFunction(log(trainReg$restricted_residual_life), trainPredlr[,1], trainReg$ipcw)
  testLoss = lossFunction(log(testReg$restricted_residual_life), testPredlr[,1], testReg$ipcw)
  
  #Create training data set for c-index
  error_train = data.frame(id=trainReg$id, true_value=log(trainReg$restricted_residual_life),
                           pred_value=trainPredlr[,1], delta_star=trainReg$delta_star)
  
  #Create testing data set for c-index
  error_test = data.frame(id=testReg$id, true_value=log(testReg$restricted_residual_life),
                          pred_value=testPredlr[,1], delta_star=testReg$delta_star)
  
  #Compute c-index
  trainConcord = compute_Concord(error_train)
  testConcord = compute_Concord(error_test)
  
  return(list(trainLoss,testLoss,trainConcord,testConcord))
}


##########################  FUNCTION TO CREATE ZETA VECTORS ########################## 
#Simulation index: i
#Data set of patients at risk at prediction time t: simDF_t
#Survival time model: surv_model ("aft", "cox")
zetaFunction = function(i, simDF_t, surv_model) {
  #Read window-specific context vectors \psi(\tau)
  contVecDF = read.table(paste0(contextVec_path, surv_model, "/contextVec", i, ".csv"), header=T, sep=",")
  contVecDF = subset(contVecDF, select=-c(Y))
  contVecDF = contVecDF[,colSums(contVecDF!=0) > 0]
  
  #Read FPCA vectors \zeta^{(F)}(\tau)
  fpcaDF = read.table(paste0(fpca_path, surv_model, "/fpca", i, ".txt"), sep=",", header=T)
  
  #Read linear regression vectors \zeta^{(S)}(\tau)
  lrDF = read.table(paste0(lm_path, surv_model, "/linReg", i, ".txt"), sep=",", header=T)
  
  #Read mixed effects vectors \zeta^{(M)}(\tau)
  mixedDF = read.table(paste0(lm_path, surv_model, "/mixedEff", i, ".txt"), sep=",", header=T)
  
  #Create baseline vectors \zeta^{(B)}(\tau)
  baselineDF = simDF_t[which(simDF_t$block==1), c("id","restricted_residual_life","delta_star","base_cov","long_cov")]
  
  #Create last-value carried forward vectors \zeta^{(L)}(\tau)
  lastValDF = subset(simDF_t %>% group_by(id) %>% top_n(n=1, wt=meas_time), select=c(id,restricted_residual_life,delta_star,base_cov,long_cov))
  
  #Create average vectors \zeta^{(A)}(\tau)
  avgDF = merge(simDF_t %>% group_by(id) %>% summarise_at(vars(long_cov), mean), subset(baselineDF, select=-c(long_cov)), by="id")
  
  return(list(baselineDF, lastValDF, avgDF, lrDF, mixedDF, fpcaDF, contVecDF))
}


##########################  FUNCTION TO MIN-MAX SCALE DATA ########################## 
#Vector of continuous variables: x
minMaxScale = function(x) {
  #Min-max scale data
  return( (x- min(x)) / (max(x)-min(x)) )
}


##########################  FUNCTION TO COMPUTE PERFORMANCE METRIC FOR GIVEN ITERATION ########################## 
#Simulation index: ind
#Dataframe of survival/censoring times: timeDF
#Dataframe of covariates: covDF
iterFN = function(ind, timeDF, covDF) {
  library(dplyr)
  library(glmnet)
  set.seed(5678)
  
  #Merge survival/censoring time data & covariate data
  simDF = merge(timeDF, covDF, by="id")
  
  #Min-max scale longitudinal covariate
  simDF$long_cov = minMaxScale(simDF$long_cov)
  
  #Keep only measurements taken prior to t on patients at risk at t
  simDF_t = simDF[which((simDF$meas_time<t) & (simDF$Y>t)),]
  
  #Create restricted residual life variable
  simDF_t$restricted_residual_life = simDF_t$Y - t
  
  #Create list of \zeta vectors for the LSTM-GLM & 6 dynamic transformed MRL models
  zetaVecs = zetaFunction(ind, simDF_t, surv_model)
  
  #Read in training & testing IDs & IPCWs
  idTrain = read.table(paste0(ipcw_path, surv_model, "/trainID", ind, ".csv"), sep=",", header=T)
  idTest = read.table(paste0(ipcw_path, surv_model, "/testID", ind, ".csv"), sep=",", header=T)
  
  #Calculate training & testing performance metrics for LSTM-GLM & each dynamic transformed MRL model
  metric_list = vector(mode="list", length=length(zetaVecs))
  for(k in 1:length(zetaVecs)) {
    #Separate data set into training and testing, and merge IPCWs
    trainReg = merge(idTrain, zetaVecs[[k]], by="id", all.x=T, all.y=F)
    testReg = merge(idTest, zetaVecs[[k]], by="id", all.x=T, all.y=F)
    
    #Compute training & testing performance metrics
    metric_list[[k]] = dynTrans_metrics(trainReg, testReg) 
  }
  
  #Calculate training & testing performance metrics for LSTM-NN
  nnTrain = read.table(paste0(lstmNN_path, surv_model, "/trainPred", ind, ".csv"), sep=",", header=T)
  nnTest = read.table(paste0(lstmNN_path, surv_model, "/testPred", ind, ".csv"), sep=",", header=T)
  nnMetrics = lstmNN_metrics(nnTrain, nnTest)
  
  #Return metrics
  metric_list[[length(metric_list)+1]] = nnMetrics
  return(metric_list)
}


############################### MAIN ##############################
#Create matrices to store training & testing losses
trainLoss_mat = matrix(NA, nrow=iter, ncol=8)
colnames(trainLoss_mat) = c("baseline", "last_value", "average", "lin_regress", "mixed_effects", "fpca", "lstm_glm", "lstm_nn")
testLoss_mat = matrix(NA, nrow=iter, ncol=8)
colnames(testLoss_mat) = c("baseline", "last_value", "average", "lin_regress", "mixed_effects", "fpca", "lstm_glm", "lstm_nn")

#Create matrices to store training & testing c-indexes
trainCindex_mat = matrix(NA, nrow=iter, ncol=8)
colnames(trainCindex_mat) = c("baseline", "last_value", "average", "lin_regress", "mixed_effects", "fpca", "lstm_glm", "lstm_nn")
testCindex_mat = matrix(NA, nrow=iter, ncol=8)
colnames(testCindex_mat) = c("baseline", "last_value", "average", "lin_regress", "mixed_effects", "fpca", "lstm_glm", "lstm_nn")

#Input covariate data
covDF = read.table(paste0(data_path, "covariates.csv"), sep=",", header=T)

#Input survival time data for each simulation
time_list = vector(mode="list", length=iter)
for(i in 1:iter) {
  time_list[[i]] = read.table(paste0(data_path, surv_model, "/survTimes", i, ".csv"), sep=",", header=T)
}

#Set up parallelization
myCluster = makeCluster(numCores, type="PSOCK") 
registerDoParallel(myCluster)

#Compute training/testing losses & c-indexes for each simulation
results = foreach(i=1:iter) %dopar% {iterFN(i, time_list[[i]], covDF)}
stopCluster(myCluster)

#Write results for each simulation
for(i in 1:iter) {
  for(j in 1:8) {
    trainLoss_mat[i,j] = results[[i]][[j]][[1]]
    testLoss_mat[i,j] = results[[i]][[j]][[2]]
    trainCindex_mat[i,j] = results[[i]][[j]][[3]]
    testCindex_mat[i,j] = results[[i]][[j]][[4]]
  }
}
write.table(trainLoss_mat, paste0(output_path, surv_model, "/train_loss.csv"), sep=",", row.names=F)
write.table(testLoss_mat, paste0(output_path, surv_model, "/test_loss.csv"), sep=",", row.names=F)
write.table(trainCindex_mat, paste0(output_path, surv_model, "/train_cIndex.csv"), sep=",", row.names=F)
write.table(testCindex_mat, paste0(output_path, surv_model, "/test_cIndex.csv"), sep=",", row.names=F)



